---
title: "Regression: Home Price Prediction"
author: "Advanced Modelling, Master in Computational Social Science"
date: 'UC3M, 2024'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---


```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri(file.path("uc3m.jpg")), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;',
               width="600",
               height="80")
```

# Motivation

In this computer lab we are going to develop analytical models to predict the final prices for homes

And also: can we find the most important variables affecting prices? 

In that way, we can

- Detect better opportunities

- Understand better the real state market

<center>
<img src="zillow.png" width="400"/>
</center>

## The data set

The dataset Sacramento contains house and sale price data for 932 homes in Sacramento CA. These are real state transactions reported over a five-day period.

The original data were obtained from the website for the SpatialKey software. 

Google was used to fill in missing/incorrect data.

```{r}
# delete everything
rm(list=ls()) 

library(leaflet)
library(tidyverse)
library(MASS)
library(caret)
```

Load the dataset:

```{r}
data(Sacramento)
```

A very brief summary

```{r}
names(Sacramento)
dim(Sacramento)
str(Sacramento)
summary(Sacramento)
```

## Splitting

```{r}
in_train <- createDataPartition(log(Sacramento$price), p = 0.75, list = FALSE)  # 75% for training
training <- Sacramento[ in_train,]
testing <- Sacramento[-in_train,]
nrow(training)
nrow(testing)

```

# Some exploratory analysis

Plot the training locations, including the price and sqft information

```{r}
color_pal <- colorNumeric(palette = "RdYlBu", domain = training$price, reverse=T)

map = leaflet(training) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  setView(lng=median(training$longitude), lat=median(training$latitude), zoom = 9) %>%
  # Marcas mediante circulos
  addCircles(lng = ~longitude, 
             lat = ~latitude,
             radius = ~sqrt(sqft)*2,
             color = ~color_pal(price)) %>% 
  addLabelOnlyMarkers(
    lng = median(training$longitude)-.6, lat = median(training$latitude)+.3,
    label = "Price and size",
    labelOptions = labelOptions(textsize = "20px",noHide = T, textOnly = T)) 

map %>% addLegend(position="bottomleft", pal = color_pal, values = ~price, bins=4)

```

**statistics**
lm(price ~ long^2 + lat^2)

**machine learning**
rf(price ~ lon + lat)

How many variables are displayed in this 2-D graph?

Home prices:

```{r}
# in machine learning our target variable is logarithmic one
training %>% ggplot(aes(x=price)) + geom_density(fill="navyblue") + scale_x_log10()
```

```{r}
# original target removing logarithm (statistics, normally it is positive distribution)
training %>% ggplot(aes(x=price)) + geom_density(fill="navyblue")
```

Prices are somehow symmetric but with high variability

Take a look at the data

```{r}
summary(training)
```

# Advanced Regression

Which are the most correlated variables with price?

```{r}
corr_price <- sort(cor(training[,c(3,4,5,7,8,9)])["price",], decreasing = T) # correlation of each variables with the price
# correlation between price and logitude and latitude is low because the relationship is not linear
corr=data.frame(corr_price)
ggplot(corr,aes(x = row.names(corr), y = corr_price)) + 
  geom_bar(stat = "identity", fill = "lightblue") + 
  scale_x_discrete(limits= row.names(corr)) +
  labs(x = "Predictors", y = "Price", title = "Correlations") + 
  theme(plot.title = element_text(hjust = 0, size = rel(1.5)),
        axis.text.x = element_text(angle = 45, hjust = 1))

```


## Multiple regression:

```{r}
linFit <- lm(log(price) ~ beds + baths + log(sqft) + type + latitude*longitude, data=training)
# we are using logarithm of price and the size of the house
# size is basically squared meter so we can erase the square using the logarithm
summary(linFit)
```

R2 is roughly 60%, some variables non-significant

Prediction

```{r}
pr.multiple = exp(predict(linFit, newdata=testing))
# Since the result is in logarithmic format so we put exponential to get the result in dollar or euro format
cor(testing$price, pr.multiple)^2
```

## Model Selection

Best subset regression: exponential with $p$, because it returns the best subset regression for each $k=1,\ldots,p$

```{r}
install.packages("olsrr")
library(olsrr)

model = log(training$price) ~ training$beds + training$baths*training$type + log(training$sqft) + training$latitude*training$longitude + I(training$longitude^2) + I(training$latitude^2)

linFit <- lm(model)

ols_step_best_subset(linFit) 
```

4 or 5 variables is ok

But we need more practical methods when dimension ($p/n$) is high 

```{r}
ols_step_forward_p(linFit) # forward based on p-value
plot(ols_step_forward_p(linFit))
```

```{r}
ols_step_forward_aic(linFit) # forward based on AIC

```

```{r}
ols_step_backward_aic(linFit) # backward AIC

```

```{r}
ols_step_both_aic(linFit) # stepwise AIC

```

This model with 8 variables seems reasonable: it's simple and explains well the price variability

```{r}
linFit <- lm(log(price) ~ beds + baths*type + log(sqft) + type + I(longitude^2)+I(latitude^2), data=training)
summary(linFit)
```

Does the beds coefficient make any sense? why?

## Prediction

```{r}
predictions <- exp(predict(linFit, newdata=testing))
cor(testing$price, predictions)^2
RMSE <- sqrt(mean((predictions - testing$price)^2))
RMSE
```

Seems reasonable. R2 in testing is similar to that in training

Now we need to try some advanced regression models: statistical learning + machine learning

But first... how do we know we are doing well?

# A benchmark

Always consider a benchmark model (or a reference)

For instance, we can predict all the new home prices as the average price in the training set

```{r}
mean(training$price)

# This is equivalent to
benchFit <- lm(price ~ 1, data=training)
predictions <- predict(benchFit, newdata=testing)
cor(testing$price, predictions)^2
RMSE <- sqrt(mean((predictions - testing$price)^2))
RMSE
```

Note the benchmark performs worse than multiple regression... 

But in noisy and difficult applications, the benchmark is difficult to beat!

## Statistical Learning tools

Let's use the caret package with CV

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5, repeats = 1)

print(ctrl)
```

But with some feature engineering first: note beds, baths, and sqft are correlated

```{r}
training$bedsperbath = training$beds/training$baths # the number of beds per baths 
# we are using the correlinearity
testing$bedsperbath = testing$beds/testing$baths
```

Also remember longitude and latitude are related in a non-linear way with price

Hence we can add some interactions

```{r}
# log(price) makes the price more linear
# We only use I() in the statistical method
linFit <- lm(log(price) ~ bedsperbath*type + log(sqft) + latitude:longitude + I(latitude^2)+ I(longitude^2), data=training)

# The syntax x1:x2 tells R to include an interaction term between
# x1 and x2. The syntax x1*x2 simultaneously includes x1, x2,
# and the interaction term x1:x2 as predictors; it is a shorthand for
# x1+x2+x1:x2

summary(linFit)
# R squared is about 60% and it is interpreted as a good model
```

R2 is roughly 60%, but many variables non-significant

Consider then these two models:

```{r}
# We can try many other models (intelligence modelling) - we can do this on the statistical modelling

# Model S = "S"imple model - basically linear model
ModelS = log(price) ~ beds + baths + log(sqft) + type + latitude + longitude

# Model F = More complex model
ModelF = log(price) ~ bedsperbath*type + log(sqft) + latitude:longitude + I(latitude^2)+ I(longitude^2)
```

Do we want to include city and zip as predictors?

Let's try, but note in the testing set we can have factor levels not presented in the training

Need to fix that: update factor levels so that prediction works

```{r}
# Since there are too many city and zip, it is not recommendable to use those two variables for the model
# It will make the dummies for each value in the city and zip 
# We are computing the levels of city using complete data set (all the names are included)
# Too small amount of levels would not be make a valuable results
levels(training$city) <- levels(Sacramento$city)
levels(training$zip) <- levels(Sacramento$zip)
```

And now, we are ready to include city and zip as predictors

```{r}
ModelFF = log(price) ~ city + zip + bedsperbath*type + log(sqft) + latitude:longitude + I(latitude^2)+ I(longitude^2)
```

From now, we are going to try many models, so it's convenient to create a data frame with all the predictors (prices in logs)

```{r}
test_results <- data.frame(price = log(testing$price))
```

### Linear regression

Train

```{r}
# R squared is 63%
lm_tune <- train(ModelS, data = training, 
                 method = "lm", 
                 preProc=c('scale', 'center'),
                 trControl = ctrl)
lm_tune
```

Predict

```{r}
test_results$lm <- predict(lm_tune, testing)
postResample(pred = test_results$lm,  obs = test_results$price)
```

Visualization

```{r}
# If we have the black point very far away from the line, then this house means ,
# for example if observed price is lower than prediced market price then we should "buy" it
qplot(test_results$lm, test_results$price) + 
  labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
  lims(x = c(10, 15), y = c(10, 15)) +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

Some small bias

### Overfitted linear regression

Train

```{r}
alm_tune <- train(ModelFF, data = training, 
                  method = "lm", 
                  preProc=c('scale', 'center'),
                  trControl = ctrl)
```

Predict

```{r}
test_results$alm <- predict(alm_tune, testing)
postResample(pred = test_results$alm,  obs = test_results$price)

```

Is overfitting benign or dangerous to predict?

Visualization

```{r}
qplot(test_results$alm, test_results$price) + 
  labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
  lims(x = c(10, 15), y = c(10, 15)) +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

Less bias

### Forward regression

Train

```{r}
# At first we have 4 variables and after with 5 and subsequently 6 variables we have less errors
for_tune <- train(ModelF, data = training, 
                  method = "leapForward", 
                  preProc=c('scale', 'center'),
                  tuneGrid = expand.grid(nvmax = 4:10),
                  trControl = ctrl)

for_tune
plot(for_tune)
```

Which variables are selected?

```{r}
coef(for_tune$finalModel, for_tune$bestTune$nvmax)

```

Predict

```{r}
test_results$frw <- predict(for_tune, testing)
postResample(pred = test_results$frw,  obs = test_results$price)

```

Visualization

```{r}
qplot(test_results$frw, test_results$price) + 
  labs(title="Forward Regression Observed VS Predicted", x="Predicted", y="Observed") +
  lims(x = c(10, 15), y = c(10, 15)) +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

some bias, very similar to lm

### Backward regression

Train

```{r}
back_tune <- train(ModelF, data = training, 
                   method = "leapBackward", 
                   preProc=c('scale', 'center'),
                   tuneGrid = expand.grid(nvmax = 4:10),
                   trControl = ctrl)
back_tune
plot(back_tune)
```

which variables are selected?

```{r}
coef(back_tune$finalModel, back_tune$bestTune$nvmax)
```

Predict

```{r}
test_results$bw <- predict(back_tune, testing)
postResample(pred = test_results$bw,  obs = test_results$price)
```

Visualize

```{r}
qplot(test_results$bw, test_results$price) + 
  labs(title="Backward Regression Observed VS Predicted", x="Predicted", y="Observed") +
  lims(x = c(10, 15), y = c(10, 15)) +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

some bias, very similar to lm

### Stepwise regression

```{r}
step_tune <- train(ModelF, data = training, 
                   method = "leapSeq", 
                   preProc=c('scale', 'center'),
                   tuneGrid = expand.grid(nvmax = 4:10),
                   trControl = ctrl)
plot(step_tune)

# which variables are selected?
coef(step_tune$finalModel, step_tune$bestTune$nvmax)

test_results$seq <- predict(step_tune, testing)
postResample(pred = test_results$seq,  obs = test_results$price)

```

### Ridge regression

We need to select a greed for the lambda hyper-parameter

```{r}
# the grid for lambda
ridge_grid <- expand.grid(lambda = seq(0, .1, length = 100))

# train
ridge_tune <- train(ModelF, data = training,
                    method='ridge',
                    preProc=c('scale','center'),
                    tuneGrid = ridge_grid,
                    trControl=ctrl)
plot(ridge_tune)

# the best tune
ridge_tune$bestTune
coef(ridge_tune$finalModel$param) # getting the optimal betas # how to get the cofficient from elasticnet

# prediction
test_results$ridge <- predict(ridge_tune, testing)

postResample(pred = test_results$ridge,  obs = test_results$price)
```

Similar results but easier to use

### The Lasso

We need to select a greed for the lambda hyper-parameter (called fraction in this case)

```{r}
lasso_grid <- expand.grid(fraction = seq(.01, 1, length = 100))

lasso_tune <- train(ModelF, data = training,
                    method='lasso',
                    preProc=c('scale','center'),
                    tuneGrid = lasso_grid,
                    trControl=ctrl)
plot(lasso_tune)

lasso_tune$bestTune

test_results$lasso <- predict(lasso_tune, testing)
postResample(pred = test_results$lasso,  obs = test_results$price)

```

Insights?

### Elastic Net

Let's check the names for the hyper-parameters

```{r}
modelLookup('glmnet')
```

```{r}
elastic_grid = expand.grid(alpha = seq(0, .2, 0.01), lambda = seq(0, .1, 0.01))

glmnet_tune <- train(ModelF, data = training,
                     method='glmnet',
                     preProc=c('scale','center'),
                     tuneGrid = elastic_grid,
                     trControl=ctrl)

plot(glmnet_tune)
glmnet_tune$bestTune

test_results$glmnet <- predict(glmnet_tune, testing)

postResample(pred = test_results$glmnet,  obs = test_results$price)

```


## Machine Learning tools

### kNN

Let's chech the names for hyper-parameters

```{r}
modelLookup('kknn')
# 3 hyper-parameters: kmax, distance, kernel
# kmax: number of neighbors considered
# distance: parameter of Minkowski distance (p in Lp)
# kernel: "rectangular" (standard unweighted knn), "triangular", "epanechnikov" (or beta(2,2)), "biweight" (or beta(3,3)), "tri- weight" (or beta(4,4)), "cos", "inv", "gaussian", "rank" and "optimal".

```

Train: because ML models are non-linear, we can use simpler formulas

```{r}
knn_tune <- train(ModelS, 
                  data = training,
                  method = "kknn",   
                  preProc=c('scale','center'),
                  tuneGrid = data.frame(kmax=c(11,13,15,19,21),distance=2,kernel='optimal'),
                  trControl = ctrl)
plot(knn_tune)

test_results$knn <- predict(knn_tune, testing)

postResample(pred = test_results$knn,  obs = test_results$price)
```

### Random Forests

```{r}
rf_tune <- train(ModelS, 
                 data = training,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(1,3,5,7)), 
                 # m=3 then we are considering only 3 columns randomly selected
                 # medium number is squared root of number of columns
                 importance = TRUE)

plot(rf_tune)

test_results$rf <- predict(rf_tune, testing)

postResample(pred = test_results$rf,  obs = test_results$price)
```

Variable importance and pdp:

```{r}
library(pdp) # we can find the most important variable for the random forest

plot(varImp(rf_tune, scale = F), scales = list(y = list(cex = .95)))

partial(rf_tune, pred.var = "sqft", plot = TRUE, rug = TRUE)
# The price difference between two houses at first size part is huge but when the size is bigger then their price becomes tiny.
# When the house size is too small then the pattern does not fit with others so we can ignore the result from those data

```

## Gradient Boosting

```{r}
xgb_tune <- train(ModelS, 
                  data = training,
                  method = "xgbTree",
                  preProc=c('scale','center'),
                  objective="reg:squarederror",
                  trControl = ctrl,
                  tuneGrid = expand.grid(nrounds = c(500,1000), max_depth = c(5,6,7), eta = c(0.01, 0.1, 1),
                                         gamma = c(1, 2, 3), colsample_bytree = c(1, 2),
                                         min_child_weight = c(1), subsample = c(0.2,0.5,0.8)))

qtest_results$xgb <- predict(xgb_tune, testing)

postResample(pred = test_results$xgb,  obs = test_results$price)

plot(varImp(xgb_tune, scale = F), scales = list(y = list(cex = .95)))

partial(xgb_tune, pred.var = "sqft", plot = TRUE, rug = TRUE)

```

## Neural Networks

In Caret, you can add up to 3 hidden layers, but with the original package (neuralnet) you could add even more.

```{r}
nn_tune <- train(ModelS, 
                 data = training,
                 method = "neuralnet",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 tuneGrid = expand.grid(layer1 = c(4, 2), # first layer 4 nodes
                                        layer2 = c(2, 1, 0), # second layer 2 nodes
                                        layer3 = c(0))) # third layer 0 nodes

test_results$nn <- predict(nn_tune, testing)

postResample(pred = test_results$nn,  obs = test_results$price)

plot(varImp(nn_tune, scale = F), scales = list(y = list(cex = .95)))

partial(nn_tune, pred.var = "sqft", plot = TRUE, rug = TRUE)

```


# Ensemble

Let's summarize the MAE for all the tools

```{r}
apply(test_results[-1], 2, function(x) mean(abs(x - test_results$price)))

```

Ensemble
: **Best model today is not always a best model for the tomorrow. So we choose 3 best models in average and aggregate them to make a best model even in the future.**

```{r}
# Combination
test_results$comb = (test_results$alm + test_results$knn + test_results$xgb)/3

postResample(pred = test_results$comb,  obs = test_results$price)

```

It seems a good choice

# Final predictions


```{r}
yhat = exp(test_results$comb)

head(yhat) # show the prediction for 6 home prices

hist(yhat, col="lightblue")

```

Take care: asymmetric distribution in price predictions

# Prediction Intervals: conformal prediction

The errors are more symmetric

```{r}
y = exp(test_results$price)
error = y-yhat
hist(error, col="lightblue")
```

Because ML tools do not provide prediction intervals, we can split the testing set in two parts: one to measure the size of the noise, and the other one to compute the intervals from that size

Let's use the first 100 homes in testing to compute the noise size

```{r}
noise = error[1:100]
```

Prediction intervals: let's fix a 90% confidence

```{r}
lwr = yhat[101:length(yhat)] + quantile(noise,0.05, na.rm=T)
upr = yhat[101:length(yhat)] + quantile(noise,0.95, na.rm=T)
```

Performance using the last prices in yhat

```{r}
predictions = data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)

predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))

# how many real observations are out of the intervals?
mean(predictions$out==1)
```

```{r}
ggplot(predictions, aes(x=fit, y=real))+
  geom_point(aes(color=out)) + theme(legend.position="none") +
  xlim(20000, 1000000) + ylim(20000, 1000000)+
  geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  labs(title = "Prediction intervals", x = "prediction",y="real price")

```

